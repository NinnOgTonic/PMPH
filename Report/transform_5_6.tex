\section{Transformation 5 $\rightarrow$ 6: Coalesced access}

With a working and well-structured naive Cuda implementation, the next step was to optimise it to use coalesced access whenever possbile. To this end, we have written and read from arrays at consecutive indices by consecutive Cuda thread ids whenever possible. In doing this, we have effectively transposed (in-place during assignment) most (if not all) of the arrays from their initial versions.\n
When an array must be arranged one way in one place and transposed in another, we have transposed the array locally in a kernel into shared memory. We did this for kernels 0 and 5, but ended up reversing the change for kernel 5 (more below).\n
To take more advantage of the coalesced access, we tried using various block sizes for the kernels. The block size of kernel 0 remained the same in the tests, namely 32x32, since we use shared memory in this and cannot spawn more threads per core. The table below shows the timings of individual kernels at one-dimensional block sizes 32, 64, 128, 256 and 512.

\begin{figure}[H]
    \centering
\begin{Verbatim}
Block size:        32        64       128       256       512

Kernel   0:    149963
Kernel   1:      2927      2517      2516      2497      2455
Kernel   2:     18333     18504     18582     18622     18644
Kernel   3:     93541     53443     36580     37026     37070
Kernel   4:    132734     96662     85957     91119     92739
Kernel   5:    143290     86995     70856     71047     71148
Kernel   6:      2951      2478      2408      2412      2351
Kernel   7:     18178     18291     18385     18446     18471
Kernel   8:    166507    154633    153778    154123    154369
Kernel   9:    132842     98206     87702     91106     91275
Kernel  10:      7143      6474      6470      6523      6537

Total time:   2031194   1860382   1793703   1808618   1818025
\end{verbatim}

\caption{Table of performance in micro-seconds of kernels}
\end{figure}


Looking at the table, we found that all kernels (besides 0) performed best (allowing room for variation in measurements) at block size 128.\n
Kernel 5 remains partially non-coalesced, and we did try to fix that by using shared memory and a block size of 32x32. However, the resulting kernel took \~110000 micro-seconds to execute, which was an improvement to the 143290 micro-seconds from the one-dimensional block size of 32. But the one-dimensional block size of 128 without shared memory only took 70856 microseconds, which is clearly better.\n
We suspect that a similar improvement might be made in kernel 0, again by sacrificing shared memory for a one-dimensional block size of 128 (or others), but we will not pursue this optimisation this time around.\n
Looking at the kernel timings as above, we continued to tweak on coalesced accesses, benefiting in one kernel at the cost of another, searching for the optimal configuration.\n
The final kernel timings are listed below, and the final running time is 1695149 micro-seconds (1.7 seconds).\n


\begin{figure}[H]
    \centering
\begin{tabular}{| c | c |}
    \hline
Kernel & micro-seconds \\
    \hline
6  & 2214 \\
1  & 2278 \\
10 & 5961 \\
7  & 18728 \\
2  & 18760 \\
3  & 36609 \\
8  & 37111 \\
9  & 85748 \\
4  & 85761 \\
5  & 96671 \\
0  & 156493
    \hline
\end{tabular}
\caption{Kernel timings after all optimisations}
\end{figure}

It should be noted that all timings in this section were taken October 28th, but the same program on the same server now runs significantly faster on the 31st. Before the optimal version took 1.7 seconds, but now the same version takes 0.79 seconds.\n

These optimisations caused a factor 4.8 speedup compared to the previous version (from about 3.9 seconds to 0.79 seconds).


